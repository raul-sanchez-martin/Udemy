{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "## 1. Data Source\n",
    "\n",
    "url --> https://www.cs.jhu.edu/~mdredze/datasets/sentiment/index2.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial NLTK data path: ['/home/raul/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data']\n",
      "Final NLTK data path: ['/home/raul/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data', '/media/raul/Data/nltk_data']\n"
     ]
    }
   ],
   "source": [
    "print(\"Initial NLTK data path: {0}\".format(nltk.data.path))\n",
    "      \n",
    "nltk_data_path = '/media/raul/Data/nltk_data'\n",
    "nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "print(\"Final NLTK data path: {0}\".format(nltk.data.path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load now the stop words taking into account in the exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_stopwords = '/home/raul/Documents/udemy/datas_cience_nlp/stopwords.txt'\n",
    "STOP_WORDS = set(w.rstrip() for w in open(path_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize a lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how the lemmatizer works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: hello; Lemmatized: hello\n",
      "Original: programmer; Lemmatized: programmer\n",
      "Original: computers; Lemmatized: computer\n",
      "Original: crying; Lemmatized: cry\n"
     ]
    }
   ],
   "source": [
    "for word in [\"hello\", \"programmer\", \"computers\", \"crying\"]:\n",
    "    print(\"Original: {0}; Lemmatized: {1}\".format(word, wordnet_lemmatizer.lemmatize(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we load the raw data: possitive and negative reviews of electronic articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_reviews_path = '/home/raul/Documents/udemy/datas_cience_nlp/sentiment_analysis/sorted_data_acl/electronics/positive.review'\n",
    "neg_reviews_path = '/home/raul/Documents/udemy/datas_cience_nlp/sentiment_analysis/sorted_data_acl/electronics/negative.review'\n",
    "\n",
    "positive_reviews = BeautifulSoup(open(pos_reviews_path).read(), \"lxml\")\n",
    "positive_reviews = positive_reviews.findAll('review_text')\n",
    "negative_reviews = BeautifulSoup(open(neg_reviews_path).read(), \"lxml\")\n",
    "negative_reviews = negative_reviews.findAll('review_text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the first 3 possitive reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.ResultSet'>\n",
      "[<review_text>\n",
      "I purchased this unit due to frequent blackouts in my area and 2 power supplies going bad.  It will run my cable modem, router, PC, and LCD monitor for 5 minutes.  This is more than enough time to save work and shut down.   Equally important, I know that my electronics are receiving clean power.\n",
      "\n",
      "I feel that this investment is minor compared to the loss of valuable data or the failure of equipment due to a power spike or an irregular power supply.\n",
      "\n",
      "As always, Amazon had it to me in &lt;2 business days\n",
      "</review_text>, <review_text>\n",
      "I ordered 3 APC Back-UPS ES 500s on the recommendation of an employee of mine who used to work at APC. I've had them for about a month now without any problems. They've functioned properly through a few unexpected power interruptions. I'll gladly order more if the need arises.\n",
      "\n",
      "Pros:\n",
      " - Large plug spacing, good for power adapters\n",
      " - Simple design\n",
      " - Long cord\n",
      "\n",
      "Cons:\n",
      " - No line conditioning (usually an expensive option\n",
      "</review_text>, <review_text>\n",
      "Wish the unit had a separate online/offline light.  When power to the unit is missing, the single red light turns off only when the warning sounds.  The warning sound is like a lot of sounds you hear in the house so it isn't always easy to tell what is happening\n",
      "</review_text>]\n"
     ]
    }
   ],
   "source": [
    "print(type(positive_reviews))\n",
    "print(positive_reviews[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Transformation\n",
    "\n",
    "In this section, we are going to vectorize our literal data. First, se suffle the data and take as many positive reviews as negative (there are more possitive reviews than negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.shuffle(positive_reviews)\n",
    "positive_reviews = positive_reviews[:len(negative_reviews)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we defined the `my_tokenizer` function, which tokenize an input string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_tokenizer(s):\n",
    "    \"\"\"\n",
    "    Tokenize an input string\n",
    "    \n",
    "    :param s: input string which represents several lines / paragraph\n",
    "    :return: list containing all the tokens\n",
    "    \"\"\"\n",
    "    s = s.lower()\n",
    "    tokens = nltk.tokenize.word_tokenize(s)\n",
    "    tokens = [t for t in tokens if len(t) > 2]\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens]\n",
    "    tokens = [t for t in tokens if t not in STOP_WORDS]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we first store all the features (tokens) corresponding to all the observations, and we also get the features map. We have to iterate then over both the positive and negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_index_map = {}\n",
    "current_index = 0\n",
    "\n",
    "positive_tokenized = []\n",
    "negative_tokenized = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for review in positive_reviews:\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    positive_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1\n",
    "            \n",
    "for review in negative_reviews:\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    negative_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenth of word_index_map: 11091\n"
     ]
    }
   ],
   "source": [
    "print(\"Lenth of word_index_map: {0}\".format(len(word_index_map)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the `tokens_to_vector` function, which is going to obtain the vectorized format for each observation (adding also the label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokens_to_vector(tokens, label, word_index_map):\n",
    "    \"\"\"\n",
    "    Returns the vectorized format of an observation,\n",
    "    including features and label\n",
    "\n",
    "    :param tokens: tokens corresponding to the observation\n",
    "    :param label: label of the observation\n",
    "    :param word_index_map: dictionary which contains the strings-to-index \n",
    "    map\n",
    "    :return: numpy array containing the vectorized representation\n",
    "    of the observation (including features and label)\n",
    "    \"\"\"\n",
    "    \n",
    "    x = np.zeros(len(word_index_map)+1)\n",
    "    for t in tokens:\n",
    "        i = word_index_map[t]\n",
    "        x[i] += 1\n",
    "    x = x/x.sum()\n",
    "    x[-1] = label\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to obtain our final dataset containing all the observations in its vectorized format and save it in the variable `data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = len(positive_tokenized) + len(negative_tokenized)\n",
    "\n",
    "data = np.zeros((N, len(word_index_map) + 1))\n",
    "i = 0\n",
    "\n",
    "for tokens in positive_tokenized:\n",
    "    xy = tokens_to_vector(tokens, 1, word_index_map)\n",
    "    data[i,:] = xy\n",
    "    i += 1\n",
    "    \n",
    "for tokens in negative_tokenized:\n",
    "    xy = tokens_to_vector(tokens, 0, word_index_map)\n",
    "    data[i,:] = xy\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we split our data in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.shuffle(data)\n",
    "\n",
    "X = data[:, :-1]\n",
    "Y = data[:, -1]\n",
    "Xtrain = X[:-100, ]\n",
    "Xtest = X[-100:, ]\n",
    "Ytrain = Y[:-100, ]\n",
    "Ytest = Y[-100:, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training a Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.68\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(Xtrain, Ytrain)\n",
    "print(\"Accuracy: {0}\".format(model.score(Xtest, Ytest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we try to interpret the meaning of each word corresponding to the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item -0.951794126417\n",
      "poor -0.756664646438\n",
      "look 0.558818507452\n",
      "money -1.0673566248\n",
      "junk -0.559994220637\n",
      "unit -0.723025994661\n",
      "returned -0.78973086928\n",
      "perfect 0.956468267869\n",
      "sound 1.10385206621\n",
      "'ve 0.798391784631\n",
      "company -0.551715300813\n",
      "excellent 1.32142262447\n",
      "then -1.14804101983\n",
      "cable 0.822756873881\n",
      "week -0.730898444164\n",
      "value 0.560089392122\n",
      "memory 0.980963258223\n",
      "quality 1.58839854932\n",
      "picture 0.572354555612\n",
      "refund -0.625908994638\n",
      "bit 0.663082241948\n",
      "expected 0.550221338526\n",
      "tried -0.799445271553\n",
      "try -0.679510192992\n",
      "speaker 0.83212845677\n",
      "warranty -0.613773953173\n",
      "happy 0.607481542225\n",
      "space 0.577802709344\n",
      "love 1.17363432721\n",
      "radio -0.561903719949\n",
      "little 1.01766258733\n",
      "doe -1.14714117698\n",
      "stopped -0.543420542325\n",
      "month -0.847680312154\n",
      "comfortable 0.637361759049\n",
      "card -0.517436835623\n",
      "you 1.16336809261\n",
      "pretty 0.810205838129\n",
      "support -0.822755883536\n",
      "easy 1.67880350844\n",
      "price 2.7144396971\n",
      "return -1.18455962988\n",
      "bad -0.757009821991\n",
      "home 0.504365586163\n",
      "ha 0.796669805859\n",
      "laptop 0.614367283195\n",
      "n't -2.05011654044\n",
      "time -0.674492554271\n",
      "lot 0.734188731478\n",
      "fast 0.842376073623\n",
      "hour -0.578425368885\n",
      "highly 0.961617362864\n",
      "buy -0.858999307186\n",
      "wa -1.62550528035\n",
      "recommend 0.659550393694\n",
      "customer -0.595961101856\n",
      "terrible -0.51665884344\n",
      "paper 0.614863112842\n",
      "using 0.633371178985\n",
      "waste -1.0348707559\n",
      "pro 0.513303218646\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "for word, index in word_index_map.items():\n",
    "    weight = model.coef_[0][index]\n",
    "    if abs(weight) > threshold:\n",
    "        print(word, weight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:datascience3]",
   "language": "python",
   "name": "conda-env-datascience3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
